{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS_em_hg6bWX"
      },
      "source": [
        "# TABLE OF CONTENT :\n",
        "\n",
        "\n",
        "### 1.   Why CNN's ?\n",
        "### 2.   Convolutional Neural Networks\n",
        "    2.1  Convolution Layer \n",
        "          2.1.1  What are Filters ?\n",
        "          2.1.2  Edge Detection Filters\n",
        "          2.1.3  Why Filters ?   \n",
        "    2.2   Pooling Layer \n",
        "    2.3   Fully connected Layer 18\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7BN2L2DupDH"
      },
      "source": [
        "# Why CNN's ?\n",
        "## Building a quick smile detector using OpenCV\n",
        "OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. In simple language it is library used for Image Processing. It is mainly used to do all the operation related to Images.\n",
        "\n",
        "we are going to be using opencvs pre built \"models\" to build a smile detector. \n",
        "here's the code we are going to be using\n",
        "This chunk of code is taken from geeksforgeeks. you could also find simple tutorials for opencv on the official documentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY1WKLiYvh4t"
      },
      "source": [
        "import cv2\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
        "# eye_cascade = cv2.CascadeClassifier('./haarcascade_eye.xml')\n",
        "smile_cascade = cv2.CascadeClassifier('./haarcascade_smile.xml')\n",
        "\n",
        "\n",
        "def detect(gray, frame):\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), ((x + w), (y + h)), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        roi_color = frame[y:y + h, x:x + w]\n",
        "        smiles = smile_cascade.detectMultiScale(roi_gray, 1.8, 20)\n",
        "\n",
        "        for (sx, sy, sw, sh) in smiles:\n",
        "            cv2.rectangle(roi_color, (sx, sy), ((sx + sw), (sy + sh)), (0, 0, 255), 2)\n",
        "    return frame\n",
        "\n",
        "\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # Captures video_capture frame by frame\n",
        "    _, frame = video_capture.read()\n",
        "\n",
        "    # To capture image in monochrome\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    # calls the detect() function\n",
        "    canvas = detect(gray, frame)\n",
        "    # Displays the result on camera feed\n",
        "    cv2.imshow('Video', canvas)\n",
        "\n",
        "    # The control breaks once q key is pressed\n",
        "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the capture once all the processing is done.\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjRs0LMsJoij"
      },
      "source": [
        "## TO-DO:\n",
        "load another haarcascade from this github [repo](https://github.com/opencv/opencv/tree/master/data/haarcascades) and try to do something with it.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAb3zCtA_h3"
      },
      "source": [
        "\n",
        "\n",
        "Imagine a simple image of hand written digit 28x28 pixels\n",
        "---\n",
        "<img src=\"https://qph.fs.quoracdn.net/main-qimg-17d779fffc5ce42e3c2399a05de2dd70\" width=\"40%\" >\n",
        "\n",
        "\n",
        "Each pixel is a number between 0 and 255,\n",
        "so when we try to classifay the image we need :\n",
        "\n",
        "![Texte alternatifâ€¦](https://miro.medium.com/max/960/1*av47vApmzuM0AN21VaIcSA.png)\n",
        "\n",
        "   [image reference](https://ml4a.github.io/ml4a/looking_inside_neural_nets/)\n",
        "   \n",
        "   \n",
        "Your nueral network will look like this !!!\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/537/1*jYhgQ4I_oFdxgDD-AOgV1w.png\" width=\"90%\"  >\n",
        "\n",
        "   [image reference](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fexploring-activation-functions-for-neural-networks-73498da59b02&psig=AOvVaw0ci1SjGJrsq-hRqCyRS665&ust=1587492302213000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCIjhwO3L9-gCFQAAAAAdAAAAABAD)\n",
        "\n",
        "\n",
        "\n",
        "# What does CNN's do about this ?\n",
        "What a CNN does is: it extracts the feature of an image and convert it into lower dimension without loosing its characteristics by passing it to different type of layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " <img src=\"https://drive.google.com/uc?id=13jsebAiWGODXDuW-yxAEUJUwlXXRhkMd\" width=\"70%\" align=\"right\" >\n",
        " \n",
        "  [image reference](https://www.researchgate.net/figure/The-structure-of-5-layer-sub-CNN-used-in-the-unified-deep-model_fig2_303284226)\n",
        "  \n",
        " \n",
        "---------\n",
        "\n",
        "*   Convolution Layer\n",
        "*   Pooling Layer\n",
        "*   Fully Connected Layer\n",
        "---------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6UZYMvmgP-"
      },
      "source": [
        "#2.1-   Convolution Layer\n",
        "##2.1.1- Filters :\n",
        "A colored image is represented with a matrix :\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1073/1*icINeO4H7UKe3NlU1fXqlA.jpeg\" width=\"50%\"  >\n",
        "\n",
        "[image reference](https://towardsdatascience.com/understanding-images-with-skimage-python-b94d210afd23)\n",
        "\n",
        "We can turn to black and white :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvNVg6mYybEH"
      },
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "#import matplotlib.image as mpimg\n",
        "from skimage import io\n",
        "#image = io.imread(url)\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Read in the image\n",
        "image = io.imread(\"http://bigdeal.mu/wp-content/uploads/2019/02/324px-2018_BMW_420i_M_Sport_Automatic_2.0_Front.jpg\")\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJnUaaV66vbK"
      },
      "source": [
        "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "plt.imshow(gray, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T6U6cJcyfDG"
      },
      "source": [
        "\n",
        "A filter looks like this :    \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://www.manifold.net/doc/mfd9/images/convolution_filter01_00.png\" width=\"20%\"  >\n",
        "\n",
        "How to declare it ? simple it is normal matrix :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV44NiLuvaT1"
      },
      "source": [
        "import numpy as np #importing numpy\n",
        "#creating a filter \n",
        "first_filter= np.array([[1,7,1],\n",
        "                   [8,4.6,55550],\n",
        "                   [1,4,2131213123]\n",
        "                    ])\n",
        "print(first_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-4jEtdIv3p5"
      },
      "source": [
        "## How does a filter works ?\n",
        "<img src=\"https://drive.google.com/uc?id=1zCoc0re0Kw-otdZOOm1Lyq1Hw2jbQtFn\">\n",
        "\n",
        "[image reference](https://datascience.stackexchange.com/questions/29237/in-a-convolutional-neural-network-cnn-when-convolving-the-image-is-the-opera)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO3QHpq_62lf"
      },
      "source": [
        "# Filter the image using filter2D, which has inputs: (grayscale image, bit-depth, kernel) \n",
        "filtered_image = cv2.filter2D(gray, -1, first_filter)\n",
        "# You can use as much filter as you want \n",
        "plt.imshow(filtered_image, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFW4k2_k8gTE"
      },
      "source": [
        "#try this two filters togther\n",
        "filter_y = np.array([[ -1, -2, -1], \n",
        "                   [ 0, 0, 0], \n",
        "                   [ 1, 2, 1]])\n",
        "\n",
        "filter_x= np.array([[-1,0,1],\n",
        "                   [-2,0,2],\n",
        "                   [-1,0,1]\n",
        "                    ])\n",
        "\n",
        "# Filter the image using filter2D, which has inputs: (grayscale image, bit-depth, kernel)  \n",
        "#filtered_image = cv2.filter2D(gray, -1, filter_y)\n",
        "\n",
        "plt.imshow(gray, cmap='gray')\n",
        "print(\"hello\")\n",
        "filtered_image = cv2.filter2D(gray, -1, filter_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXwul0Rw9ic5"
      },
      "source": [
        "plt.imshow(filtered_image, cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aONDzb_kMC5f"
      },
      "source": [
        "## Padding and Stride :\n",
        "**Stride** refers to how many steps we are moving in each steps in convolution\n",
        "<img src=\"https://miro.medium.com/max/790/1*g0OmDI1w9KqN7Rpw6Qo8Xg@2x.gif\" width=\"50%\">\n",
        "\n",
        "[image reference](https://medium.com/datadriveninvestor/a-beginners-guide-to-convolutional-neural-networks-49384c75d1a)\n",
        "\n",
        "\n",
        "### Why do we need padding ?\n",
        "<img src=\"https://miro.medium.com/max/800/1*17TNPi4m0pBqOCGrXzU27w.gif\" width=\"50%\">\n",
        "\n",
        "[image reference](https://medium.com/datadriveninvestor/a-beginners-guide-to-convolutional-neural-networks-49384c75d1a)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54rPb-LMxYvg"
      },
      "source": [
        "[Here's a video that explains what's happening behind the scenes with opencv](https://youtu.be/hPCTwxF0qf4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QES4J6Xk8ye3"
      },
      "source": [
        "## Edge detection :\n",
        "Every image has vertical and horizontal edges which actually combining to form a image. Convolution operation is used with some filters for detecting edges.\n",
        "\n",
        "We will try to do that "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFqMiLvp92-4"
      },
      "source": [
        "# Read in the image\n",
        "image = io.imread(\"http://bigdeal.mu/wp-content/uploads/2019/02/324px-2018_BMW_420i_M_Sport_Automatic_2.0_Front.jpg\")\n",
        "plt.imshow(image)\n",
        "\n",
        "#\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "plt.imshow(gray, cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSLmp0Cg-W-_"
      },
      "source": [
        "##Feel free to modify the numbers here, to try out another filter!\n",
        "filter = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n",
        "\n",
        "print('Filter shape: ', filter.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVfUR5rv-eR3"
      },
      "source": [
        "# Defining four different filters, \n",
        "# all of which are linear combinations of the `filter` defined above\n",
        "\n",
        "# define four filters\n",
        "filter_1 = filter\n",
        "filter_2 = filter_1.T\n",
        "filter_3 = -filter_1\n",
        "filter_4 = -filter_3\n",
        "filters = np.array([filter_1, filter_2, filter_3, filter_4])\n",
        "\n",
        "# For an example, print out the values of filter 1\n",
        "print('Filter 1: \\n', filter_1,'Filter 2: \\n', filter_2,'Filter 3: \\n', filter_3,'Filter 4: \\n', filter_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCn4suqv-vIR"
      },
      "source": [
        "# visualize all four filters \n",
        "# gives a great way to see what does the filters do\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "for i in range(4):\n",
        "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(filters[i], cmap='gray')\n",
        "    ax.set_title('Filter %s' % str(i+1))\n",
        "    width, height = filters[i].shape\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        color='white' if filters[i][x][y]<0 else 'black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se5DG8EOAn-3"
      },
      "source": [
        "## Defining a convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRlqPpTD_8VK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "    \n",
        "# define a neural network with a single convolutional layer with four filters\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self, weight):\n",
        "        super(Net, self).__init__()\n",
        "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
        "        k_height, k_width = weight.shape[2:]\n",
        "        # defines the convolutional layer, assumes there are 4 grayscale filters\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
        "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
        "        self.conv.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculates the output of a convolutional layer\n",
        "        # pre- and post-activation\n",
        "        conv_x = self.conv(x)\n",
        "        activated_x = F.relu(conv_x)\n",
        "        \n",
        "        # returns both layers\n",
        "        return conv_x, activated_x\n",
        "    \n",
        "# instantiate the model and set the weights\n",
        "weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n",
        "model = Net(weight)\n",
        "\n",
        "# print out the layer in the network\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZGWnv1GA-XW"
      },
      "source": [
        "### We want now to visualise the image after it passed into a certain number of layers by defining this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YFmw4VEBMRA"
      },
      "source": [
        "# helper function for visualizing the output of a given layer\n",
        "# default number of filters is 4\n",
        "def viz_layer(layer, n_filters= 4):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    \n",
        "    for i in range(n_filters):\n",
        "        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n",
        "        # grab layer outputs\n",
        "        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')\n",
        "        ax.set_title('Output %s' % str(i+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA59Ze_GBaDf"
      },
      "source": [
        "### Now we visulaise the images after passing by eact filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYMk6TTsBYdz"
      },
      "source": [
        "# plot original image\n",
        "plt.imshow(gray, cmap='gray')\n",
        "\n",
        "# visualize all filters\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(4):\n",
        "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(filters[i], cmap='gray')\n",
        "    ax.set_title('Filter %s' % str(i+1))\n",
        "\n",
        "    \n",
        "# convert the image into an input Tensor\n",
        "gray_img_tensor = torch.from_numpy(gray).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# get the convolutional layer (pre and post activation)\n",
        "conv_layer, activated_layer = model(gray_img_tensor.float())\n",
        "print(conv_layer)\n",
        "# visualize the output of a conv layer\n",
        "viz_layer(conv_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313sI05PCRbB"
      },
      "source": [
        "## Activation Function\n",
        "Activation functions are mathematical equations that determine the output of a neural network.\n",
        "This function helps determines whether the neurons should be activated or not, based on whether each neuronâ€™s input is relevant for the modelâ€™s prediction. \n",
        "Activation functions can be used to normlize the outputs.\n",
        "### Types of activation functions :\n",
        "\n",
        "\n",
        "*   Linear Activation Function\n",
        "*   **Non-Linear Activation Functions**\n",
        "\n",
        "### Why Non-Linear Activation Functions is used ?\n",
        "    - Non-Linear Activation Functions have a derivative,so they allow backpropagation\n",
        "    - They allow creating a deep neural network. \n",
        "### ReLU (Rectified Linear Unit):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=192rGF8tZrCeBIK4bXZgUhWdN0zUy9gTW\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l10OV-RgCfgH"
      },
      "source": [
        "# after a ReLu is applied\n",
        "print(activated_layer)\n",
        "viz_layer(activated_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7L3PcO6LqzR"
      },
      "source": [
        "# Why Filters ?\n",
        "\n",
        "Convolutional neural networks applies a filter to an input to create a feature map that summarizes the presence of detected features in the input.\n",
        "\n",
        "Convolutional neural networks do not learn a single filter; they, in fact, learn multiple features in parallel for a given input.\n",
        "\n",
        "\n",
        "Building a Convolutional layer:\n",
        "<img src=\"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/convolutional-neural-networks/conv-visualization/notebook_ims/conv_layer.gif\" width=\"100%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOro5bUPN6U"
      },
      "source": [
        "# 2.2- Pooling Layer\n",
        "Now we have as a result a stack of filtred image and more our images are complex the more filters we have, higher dimensionality means more computing ressourses to use and also can lead to **overfitting**.\n",
        "To reduce this dimentalinty we use pooling layer, one of the most used pooling layers is :\n",
        "### **Max-pool :**\n",
        "<img src=\"https://austingwalters.com/wp-content/uploads/2019/01/max-pooling.png\" width=\"50%\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_QGFw9iOq2y"
      },
      "source": [
        "#in the forward part\n",
        "self.pool = nn.MaxPool2d(2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40qaH1oO1MI"
      },
      "source": [
        "### Adding max pool layer in our model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdol1dDJhs9"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self, weight):\n",
        "        super(Net, self).__init__()\n",
        "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
        "        k_height, k_width = weight.shape[2:]\n",
        "        # defines the convolutional layer, assumes there are 4 grayscale filters\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
        "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
        "        # max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculates the output of a convolutional layer\n",
        "        # pre- and post-activation\n",
        "        convv=self.conv(x)\n",
        "        #Showing the size of the image after applying the conv layer \n",
        "        print(\"image shape after conv layer\",convv.shape)\n",
        "\n",
        "        conv_x = self.pool(convv)\n",
        "        #Showing the size of the image after applying the pool layer \n",
        "        print(\"image shape after max pool layer\",conv_x.shape)\n",
        "        activated_x = F.relu(conv_x)\n",
        "        \n",
        "        # returns both layers\n",
        "        return conv_x, activated_x\n",
        "# instantiate the model and set the weights\n",
        "\n",
        "weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n",
        "print()\n",
        "model = Net(weight)\n",
        "\n",
        "# print out the layer in the network\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM_HM-_jRd8a"
      },
      "source": [
        "### As a result :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS3u8c_iRbqc"
      },
      "source": [
        "\n",
        "# plot original image\n",
        "plt.imshow(gray, cmap='gray')\n",
        "\n",
        "\n",
        "    \n",
        "# convert the image into an input Tensor\n",
        "gray_img_tensor = torch.from_numpy(gray).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# get the convolutional layer (pre and post activation)\n",
        "conv_layer, activated_layer = model(gray_img_tensor.float())\n",
        "\n",
        "# visualize the output of a conv layer\n",
        "viz_layer(activated_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_NjT127Tz58"
      },
      "source": [
        "# 2.3- Fully connected Layer\n",
        "Now that we have converted our input image into a **feature level representaion**, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training.\n",
        "### Forward propagation :\n",
        "### Backpropagation and computing gradients:\n",
        "<img src=\"https://www.researchgate.net/publication/303744090/figure/fig3/AS:368958596239360@1464977992159/Feedforward-Backpropagation-Neural-Network-architecture.png\" width=\"60%\">\n",
        "\n",
        "[image reference](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0156338.g001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYYmmtepIoOd"
      },
      "source": [
        "### At the end we will get a similar architecture :\n",
        "<img src=\"https://adeshpande3.github.io/assets/Cover.png\">\n",
        "\n",
        "[image reference](https://medium.com/datadriveninvestor/a-beginners-guide-to-convolutional-neural-networks-49384c75d1a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3uvKFKQKlT"
      },
      "source": [
        "The code from this notebook is mainly taken from Udacity's introdution to pytorch.\n",
        "[here's the repo with all the code ](https://github.com/udacity/deep-learning-v2-pytorch)"
      ]
    }
  ]
}